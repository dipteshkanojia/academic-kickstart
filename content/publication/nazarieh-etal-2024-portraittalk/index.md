---
title: "PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation"
date: 2024-12-11T00:00:00
authors: ["Fatemeh Nazarieh", "Zhenhua Feng", "Diptesh Kanojia", "Muhammad Awais", "Josef Kittler"]
publication_types: ["3"]
abstract: "Audio-driven talking face generation is a challenging task in digital communication. Despite significant progress in the area, most existing methods concentrate on audio-lip synchronization, often overlooking aspects such as visual quality, customization, and generalization that are crucial to producing realistic talking faces. To address these limitations, we introduce a novel, customizable one-shot audio-driven talking face generation framework, named PortraitTalk. Our proposed method utilizes a latent diffusion framework consisting of two main components: IdentityNet and AnimateNet. IdentityNet is designed to preserve identity features consistently across the generated video frames, while AnimateNet aims to enhance temporal coherence and motion consistency. This framework also integrates an audio input with the reference images, thereby reducing the reliance on reference-style videos prevalent in existing approaches. A key innovation of PortraitTalk is the incorporation of text prompts through decoupled cross-attention mechanisms, which significantly expands creative control over the generated videos. Through extensive experiments, including a newly developed evaluation metric, our model demonstrates superior performance over the state-of-the-art methods, setting a new standard for the generation of customizable realistic talking faces suitable for real-world applications."
featured: false
publication: "*arXiv preprint arXiv:2412.07754*"
url_pdf: "https://arxiv.org/abs/2412.07754"
url_preprint: "https://arxiv.org/abs/2412.07754"
tags: ["audio-to-talking face generation", "diffusion models", "customization", "one-shot"]
---